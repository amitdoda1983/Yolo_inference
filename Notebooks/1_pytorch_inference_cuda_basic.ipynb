{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVWK5+nX5I+CkmhAG92bfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitdoda1983/iith_GPU_inference/blob/main/1_pytorch_inference_cuda_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Simulate by creating a random image in GPU directly using torch\n",
        "2. preprocess, inference, post process in GPU.\n"
      ],
      "metadata": {
        "id": "HlsdeFG_UCW5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh6JqYq9RBj6",
        "outputId": "1f274a6d-497a-44a1-8cb1-bd64ce9e4d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.10/dist-packages (2024.1.2)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.10/dist-packages (from pycuda) (2024.1.19)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (4.3.6)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install pycuda\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pycuda.driver as cuda\n",
        "\n",
        "class_names = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "    \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "    \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "    \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
        "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
        "    \"hair drier\", \"toothbrush\"]"
      ],
      "metadata": {
        "id": "FWFehnwoRWHm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolov5_model():\n",
        "    # Load YOLOv5 model (can be replaced with other variants like yolov5m, yolov5l)\n",
        "    model = torch.hub.load('ultralytics/yolov5:v6.2', 'yolov5s')\n",
        "    model = model.cuda()  # Move model to GPU\n",
        "    return model\n",
        "\n",
        "# Simulate DPU image buffer (this would be GPU memory in real scenario)\n",
        "def simulate_dpu_image_buffer():\n",
        "    return torch.rand((1, 3, 1280, 720), dtype=torch.float32, device='cuda')\n",
        "\n",
        "\n",
        "# Preprocess image on GPU\n",
        "def preprocess_image_gpu(image_tensor):\n",
        "    \"\"\"\n",
        "    Preprocessing for YOLOv5 model, including resizing, normalization, etc.\n",
        "    \"\"\"\n",
        "    # Resize image (if necessary) - YOLOv5 expects 640x640 input images.\n",
        "    image_tensor = torch.nn.functional.interpolate(image_tensor, size=(640, 640))  # Resize to 640x640 if needed\n",
        "\n",
        "    # Normalize the image (YOLOv5 uses the following normalization)\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406], device=image_tensor.device)  # Mean\n",
        "    std = torch.tensor([0.229, 0.224, 0.225], device=image_tensor.device)   # Std\n",
        "    image_tensor = (image_tensor / 255.0 - mean[None, :, None, None]) / std[None, :, None, None]  # Normalize image\n",
        "    return image_tensor\n",
        "\n",
        "\n",
        "# Run inference using PyTorch (YOLOv5)\n",
        "def infer_on_gpu(model, image_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(image_tensor)\n",
        "    return predictions\n",
        "\n",
        "def postprocess(predictions, conf_threshold=0.5):\n",
        "    pred = predictions[0]\n",
        "    boxes = pred[:, :4]  # Bounding box coordinates\n",
        "    confidences = pred[:, 4]  # Confidence scores\n",
        "    class_probs = pred[:, 5:]  # Class probabilities\n",
        "\n",
        "    # Get predicted class ID\n",
        "    class_ids = torch.argmax(class_probs, dim=1)\n",
        "    overall_confidences = confidences * class_probs.max(dim=1).values\n",
        "\n",
        "    keep = overall_confidences > conf_threshold\n",
        "    boxes = boxes[keep]\n",
        "    confidences = overall_confidences[keep]\n",
        "    class_ids = class_ids[keep]\n",
        "\n",
        "    # Print boxes, confidences, and class ids\n",
        "    for i in range(len(boxes)):\n",
        "        print(f\"Box {i}: {boxes[i]}, Confidence: {confidences[i].item():.2f}, Class ID: {class_ids[i].item()}\")\n",
        "\n",
        "    return boxes, confidences, class_ids"
      ],
      "metadata": {
        "id": "4PagefZrRa3A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv5 model (PyTorch)\n",
        "model = load_yolov5_model()\n",
        "\n",
        "# Simulate image buffer coming from the DPU (already in GPU memory)\n",
        "image_tensor = simulate_dpu_image_buffer()\n",
        "\n",
        "print(f'input image : {image_tensor.shape}')\n",
        "\n",
        "# Preprocess image directly on GPU\n",
        "image_tensor = preprocess_image_gpu(image_tensor)\n",
        "print(f'processed image : {image_tensor.shape}')\n",
        "\n",
        "# Run inference directly on GPU\n",
        "predictions = infer_on_gpu(model, image_tensor)\n",
        "print(f'yolo output : {predictions.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-DeHssOR2EY",
        "outputId": "512173ea-de19-41e4-c904-f5d66dc00e08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_v6.2\n",
            "YOLOv5 ðŸš€ 2024-12-8 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "/root/.cache/torch/hub/ultralytics_yolov5_v6.2/models/experimental.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_v6.2/models/common.py:602: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input image : torch.Size([1, 3, 1280, 720])\n",
            "processed image : torch.Size([1, 3, 640, 640])\n",
            "yolo output : torch.Size([1, 25200, 85])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "yolo output : torch.Size([1, 25200, 85])\n",
        "\n",
        "25200 represents the number of grid cells (each cell predicts multiple bounding boxes), and\n",
        "\n",
        "85 is the number of values predicted for each bounding box, where:\n",
        "\n",
        "4 values for the bounding box coordinates (x, y, width, height),\n",
        "\n",
        "1 value for the objectness score (confidence),\n",
        "\n",
        "80 values for the class scores (in the case of COCO dataset with 80 classes)."
      ],
      "metadata": {
        "id": "zeNpFianTzYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-process the results\n",
        "boxes, confidences, class_ids = postprocess(predictions, conf_threshold=0.01)\n",
        "\n",
        "for i, class_id in enumerate(class_ids):\n",
        "    print(f\"Class {class_names[class_id.item()]} with confidence {confidences[i].item():.2f}: {boxes[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejkf-JNuSXd4",
        "outputId": "e12c116a-cbc2-485f-e468-308995f9ff40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box 0: tensor([287.93216, 285.03326, 651.11548, 581.60620], device='cuda:0'), Confidence: 0.01, Class ID: 6\n",
            "Box 1: tensor([287.06616, 307.46484, 653.78400, 629.20648], device='cuda:0'), Confidence: 0.02, Class ID: 6\n",
            "Box 2: tensor([286.55090, 335.25571, 653.60400, 623.77753], device='cuda:0'), Confidence: 0.02, Class ID: 6\n",
            "Class train with confidence 0.01: tensor([287.93216, 285.03326, 651.11548, 581.60620], device='cuda:0')\n",
            "Class train with confidence 0.02: tensor([287.06616, 307.46484, 653.78400, 629.20648], device='cuda:0')\n",
            "Class train with confidence 0.02: tensor([286.55090, 335.25571, 653.60400, 623.77753], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}
